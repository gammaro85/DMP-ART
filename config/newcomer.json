{
     "newcomer": {
          "1.1": [
               "In this section, you need to describe how you'll collect or generate data at each stage of your research. Take your research activities plan from the proposal and think systematically: What kind of data will each experiment produce? Will you conduct measurements that generate numerical data? Will you run simulations? Are you reusing existing datasets? Be specific about each data source. For example, if you're doing experimental work, mention the instruments you'll use. If it's computational work, specify the simulation software. This helps plan proper data management throughout your project.",
               "When describing data collection, it's important to distinguish between different types. Experimental data comes from physical experiments (like sensor measurements, microscopy images, chemical analyses). Simulation data is generated computationally (like CFD results, molecular dynamics, economic models). Survey data involves human participants. Each type has different management requirements. Also, if you're working with external partners or subcontractors (common in social sciences), explain how they'll collect data and transfer it to you securely.",
               "For interdisciplinary projects at Gdańsk Tech, consider all data sources. Engineering projects might combine sensor data with simulations. Biomedical research might include both patient data and laboratory measurements. Architecture projects could involve both digital models and field observations. The key is being comprehensive - even preliminary sketches or failed experiments produce data that might need management."
          ],
          "1.2": [
               "This section requires three specific pieces of information: data types, formats, and volumes. For types, list what kind of data you'll have (numerical measurements, text documents, images, videos, code). For formats, be exact - don't just say 'spreadsheets', say 'CSV files encoded in UTF-8' or 'Excel XLSX files'. For volumes, provide concrete estimates like '50-100 GB' or even better '49.5 GB' based on calculations. Even if data will be deleted later, estimate the maximum storage needed during the project. This helps the university plan infrastructure support.",
               "To estimate data volume, consider: How many experiments will you run? What's the typical size of output files? For example: 100 experiments × 500 MB each = 50 GB of raw data. Add processed data (often 2-3x raw data size). Include temporary files created during analysis. A typical small project might generate 10-50 GB, medium projects 100-500 GB, and data-intensive projects can reach terabytes. Use present tense when describing ongoing data generation: 'The project generates approximately 2 GB of data monthly.'",
               "Different research fields at Gdańsk Tech have typical data patterns. Chemistry/Materials: spectroscopy data (MB per spectrum), microscopy images (GB per session). Civil Engineering: sensor data (GB per monitoring period), BIM models (hundreds of MB). IT/Electronics: log files (GB per experiment), simulation outputs (GB per run). Biomedical: medical images (GB per patient), genomic data (GB per sample). Understanding your field's typical data helps make realistic estimates."
          ],
          "2.1": [
               "Metadata is 'data about data' - the information that makes your data understandable and reusable by others (including future you!). Think of it in three levels: File-level metadata (who created it, when, in what format), Dataset-level metadata (what the data represents, how it was collected, units of measurement, experimental conditions), and Project-level metadata (research context, funding information, related publications). Good metadata is what transforms a file named 'test_001.csv' into understandable, reusable scientific data.",
               "Documentation complements metadata by providing human-readable explanations. Essential documentation includes: README files explaining file organization and naming conventions (e.g., 'EXP_YYYYMMDD_SampleID_Measurement.csv'), Codebooks defining every variable and its possible values, Methodology documents detailing experimental procedures or computational parameters, and Processing workflows showing how raw data becomes published results. Think of documentation as the instruction manual that would help a new PhD student understand and use your data.",
               "For metadata standards, DataCite is universally accepted and covers basics like creator, title, publication year, and resource type. However, many fields have specialized standards: DDI for social science surveys, Darwin Core for biodiversity, DICOM for medical imaging, CIF for crystallography. At Gdańsk Tech, we recommend starting with DataCite and adding discipline-specific metadata as needed. The key is consistency - pick a standard and stick to it throughout your project."
          ],
          "2.2": [
               "Data quality control ensures your data is accurate, complete, and reliable - it's different from data security (section 3.2). Start by defining quality criteria for your data: What are acceptable ranges for measurements? Which fields must always be filled? What consistency rules apply between related variables? For experimental data, this includes instrument calibration records, control measurements, and blank samples. For computational data, it means parameter validation and benchmarking against known solutions.",
               "Implement quality control throughout your workflow, not just at the end. During data collection: Calibrate instruments before each session, use control samples, document any anomalies or deviations. During data entry: Use validation rules (e.g., age must be 0-120), implement double-entry for critical data, flag potential outliers automatically. During analysis: Check for missing data patterns, verify calculations with test cases, maintain audit trails of all data transformations.",
               "For small projects like Miniatura (12-month single-person projects), quality control can be simpler but must still be systematic. Even a single researcher should document: instrument calibration dates, software versions used, any data points excluded and why. Version control is crucial - when you correct errors, don't overwrite original data. Keep versioned datasets with clear documentation of what changed between versions. This creates credibility and allows others to verify your work."
          ],
          "3.1": [
               "Storage and backup protect your research investment. At Gdańsk Tech, you have access to Microsoft Office 365 A1 for faculty license, which includes 1TB of OneDrive storage per user. This serves as both working storage (where you actively use data) and off-site backup. Always mention this institutional resource in your DMP: 'Data will be stored using the institutional Office 365 A1 for faculty license, providing cloud storage with automatic versioning and EU data residency compliance.'",
               "Follow the 3-2-1 backup rule: Keep 3 copies of important data (1 primary + 2 backups), Store them on 2 different media types (e.g., local SSD and cloud storage), Keep 1 copy off-site (OneDrive fulfills this). Specify backup frequency based on data criticality: Daily for actively collected data, weekly for processed datasets, immediately for irreplaceable data (like one-time field observations). Automate backups where possible - OneDrive can sync folders automatically.",
               "Consider the full data lifecycle in your storage plan. During collection: Store on local devices with immediate sync to cloud. During processing: Work on local copies for speed, sync results daily. For sharing with team: Use OneDrive shared folders with appropriate permissions. After project: Archive complete datasets to OneDrive and institutional repository. Remember to backup metadata and documentation together with data - they're useless separately."
          ],
          "3.2": [
               "Data security protects against unauthorized access, corruption, or loss. The level of security depends on data sensitivity. For most research data (non-personal, non-confidential), standard security includes: Password-protected computers with full-disk encryption, automatic screen locks, regular security updates. Cloud storage (OneDrive) with two-factor authentication. Secure connections (HTTPS, VPN when off-campus). Never send data through unencrypted email - use OneDrive links instead.",
               "For sensitive data (personal information under GDPR, confidential business data, pre-patent results), add extra layers: Encryption at rest and in transit (OneDrive provides this). Access control lists specifying who can view/edit data. Separate storage from other project data. Secure deletion when no longer needed (not just 'delete' but secure overwriting). Consider pseudonymization where possible - replace identifiers with codes, keeping the key separate.",
               "Physical security matters too, especially for field equipment and portable devices. Labs should have controlled access. Laptops used for fieldwork need extra precautions: BIOS passwords, encrypted drives, remote wipe capability. USB drives are particularly vulnerable - avoid them for sensitive data, or use encrypted drives only. Document your security measures in the DMP - this shows you've thought about risks and builds trust with ethics committees and funders."
          ],
          "4.1": [
               "GDPR (General Data Protection Regulation) applies whenever you process personal data of any individual in the EU. Personal data is broader than just names and addresses - it includes email addresses, IP addresses, voice recordings, photos where people can be identified, location data, and even behavioral patterns that could identify someone. At Gdańsk Tech, if your research involves any human participants, careful GDPR consideration is essential.",
               "If your project does NOT process personal data, state this clearly and specifically: 'This project will not process any personal data. All data will be collected from [instruments/simulations/publicly available non-personal sources].' Be certain - even metadata like 'data collected by: jan.kowalski@pg.edu.pl' contains personal information. For projects with only technical data (materials testing, environmental monitoring without human subjects, theoretical work), this declaration is usually straightforward.",
               "If you DO process personal data, you need: (1) Legal basis - usually consent for research, documented through ethics-approved consent forms. (2) Data minimization - collect only what's necessary for your research objectives. (3) Clear retention periods - how long will you keep the data and why. (4) Subject rights procedures - how people can access, correct, or delete their data. Consult with Gdańsk Tech's Data Protection Officer early. Remember, GDPR compliance protects both participants and researchers."
          ],
          "4.2": [
               "Data ownership and intellectual property must be clear from the start. For projects at Gdańsk Tech, the default is that data belongs to the university, but there are important nuances. Single-author projects by employees: University owns the data, but researchers have broad usage rights. Multi-partner projects: Ownership must be defined in consortium agreements before starting. Industry collaborations: Often involve shared ownership or specific usage restrictions. Student projects: Special rules apply - consult your faculty's guidelines.",
               "Choose your data license carefully - it determines how others can use your research outputs. For NCN projects, note that CC BY-SA is NOT accepted - use CC BY 4.0 instead. CC BY 4.0 (Attribution) is recommended for maximum impact: others can use your data for any purpose if they cite you. CC BY-NC restricts commercial use but can limit legitimate research uses. CC0 waives all rights - use only for truly basic data. Remember: the license you specify here in section 4.2 MUST also appear in section 5.1 - this is a common oversight.",
               "Consider all intellectual property angles: If using existing datasets, check their licenses - restrictions may cascade to your outputs. If your research might lead to patents, plan for embargo periods (typically 12-18 months) before open data release. If using commercial software, verify that you can share outputs. If collaborating internationally, consider jurisdiction differences. Document all IP considerations clearly - this prevents disputes and enables smooth collaboration."
          ],
          "5.1": [
               "Data sharing timeline must be specific, not vague. Instead of 'after project completion', write: 'Data will be made publicly available upon publication of the associated research article, expected March 2026' or 'Immediately upon upload to repository, with DOI reserved for inclusion in manuscript'. Common timelines: Immediate for non-sensitive data, upon publication for most research data, after 12-month embargo for patent applications, never for personal data that can't be anonymized. Each choice needs justification.",
               "Don't forget to repeat the license information from section 4.2 here - this is one of Piotr Krajewski's most common corrections. State clearly: 'Data will be shared under Creative Commons Attribution 4.0 International (CC BY 4.0) license, allowing any use with attribution.' Also specify HOW data will be shared: through repository (best option), on project website (not recommended alone), by request (discouraged as it depends on your availability). Repository sharing is preferred because it provides persistent access.",
               "Address any access restrictions honestly. If some data can't be shared due to privacy, commercial sensitivity, or security concerns, explain what CAN be shared: 'Raw sensor data will be openly available. Processed data with company-specific parameters will be available under NDA. Analysis scripts and aggregate results will be openly shared.' This transparency helps others understand what they can access and plan accordingly. Remember, even restricted sharing is better than no sharing."
          ],
          "5.2": [
               "Not all data needs eternal preservation - be strategic. Priority goes to: Data underlying published results (essential for reproducibility), unique observations that can't be repeated (rare events, demolished buildings, extinct species), data with potential long-term value beyond your project, and well-documented, clean datasets that others can actually use. Raw data might be too large - consider preserving processed data plus detailed processing methods instead.",
               "For Gdańsk Tech researchers, MOST Wiedzy (Bridge of Data) is your primary repository option. Always mention it first: 'Data will be deposited in MOST Wiedzy (Bridge of Data), Gdańsk University of Technology's institutional repository, which is CoreTrustSeal certified and ensures minimum 10-year preservation.' For discipline-specific needs, add: 'Alternatively, [Zenodo for interdisciplinary data / GenBank for sequences / PDB for protein structures] will be used.' Domain repositories often provide specialized tools but always have MOST Wiedzy as backup.",
               "Long-term preservation requires more than just storage. Your data package should include: Primary data files in open formats, comprehensive metadata following standards, all documentation (READMEs, codebooks, protocols), any custom software needed for data use, and clear license information. Think about usability in 10+ years: Will the formats still be readable? Is the documentation complete enough for someone with no project knowledge? This 'future-proofing' is what distinguishes archived data from merely stored data."
          ],
          "5.3": [
               "Software and tools needed to use your data must be specified precisely. Don't just write 'Python' - specify 'Python 3.8 or higher'. Don't just write 'R' - write 'R 4.0 with packages: tidyverse 1.3.1, ggplot2 3.3.3'. Include all dependencies. This precision is crucial because software behavior can change between versions, potentially making your data unusable. For standard tools, newer versions usually work, but always test compatibility.",
               "Think beyond the primary analysis software. List everything needed for the complete workflow: Operating system requirements (some tools are platform-specific), hardware requirements (especially for large datasets - 'minimum 16GB RAM for loading complete dataset'), specialized libraries or plugins with version numbers, configuration files or parameter settings, and any license requirements (is it open source or does it need a commercial license?). If you've written custom scripts, these are part of your data product.",
               "For proprietary software (MATLAB, SPSS, specialized engineering tools), always suggest open alternatives when possible: 'Data can be analyzed using MATLAB R2023a (proprietary) or GNU Octave 6.4 (open source) with minor syntax adjustments.' Provide migration guides if needed. Include example scripts showing basic data loading and analysis - this lowers barriers for reuse. Remember, in 10 years, today's standard software might be obsolete, but open formats and well-documented workflows survive."
          ],
          "5.4": [
               "Persistent identifiers, particularly DOIs (Digital Object Identifiers), make your data citable and findable forever. At Gdańsk Tech, specify: 'DOIs will be automatically assigned upon deposit in MOST Wiedzy repository.' Include timeline: 'Expected deposit date: [month/year], allowing DOI inclusion in related publications.' This automatic process removes a common barrier to FAIR data. DOIs work like ISBN numbers for books - they're unique and permanent, even if websites change.",
               "Beyond repository DOIs, use consistent identifiers throughout your research. For samples: 'Each sample labeled as PROJECT_YYYY_NNN (e.g., PRELUDIUM2024_001)'. For experiments: 'Experiment IDs follow EXPYYYYMMDD_NN format'. For datasets: Consider whether you need one DOI for everything or separate DOIs for logical subsets. Document your identification scheme in a README file. This internal consistency helps users understand relationships between files.",
               "Plan how to use DOIs in practice. In publications: Include DOIs in data availability statements ('Data available at: doi.org/...'). In citations: Cite data like publications in your reference list. In metadata: Cross-reference related outputs. If updating data post-publication, use version-specific DOIs. Remember, DOIs are for finished, published data - use version control (Git) for work in progress. This distinction helps manage the transition from active research to archived data."
          ],
          "6.1": [
               "Data management responsibilities must be assigned to specific people, not just roles. Following Piotr Krajewski's consistent guidance: 'The Principal Investigator will ensure FAIR principles implementation by describing, structuring, and preparing data for publication.' But don't stop there - specify who does what: 'PhD student X: daily data collection and metadata creation. Postdoc Y: quality control and processing. PI: overall oversight and final preparation for archiving.' Clear assignment prevents important tasks from being forgotten.",
               "FAIR principles require active management throughout the project, not just at the end. The PI ensures data is: (F)indable through proper metadata and repository deposit, (A)ccessible through standard protocols and clear licensing, (I)nteroperable using standard formats and vocabularies, (R)eusable with comprehensive documentation. These high-level responsibilities translate to daily tasks: creating metadata during experiments, organizing files systematically, documenting methods thoroughly. FAIR is a practice, not a checkbox.",
               "Include succession planning - what happens if someone leaves? 'All data management procedures will be documented in a project wiki. If the responsible person leaves, their supervisor assumes their data duties. Critical information (passwords, file locations, decisions made) stored in institutional systems, not personal accounts.' For single-person projects (Miniatura), designate a colleague as backup contact. This planning shows maturity and ensures data remains accessible regardless of personnel changes."
          ],
          "6.2": [
               "Following Piotr Krajewski's standard formulation: 'The only additional resource required is time for FAIR procedures assessment and data quality assurance.' But be specific about time allocation: 'Estimated at 5% of total research time - approximately 2 hours per week for data organization, metadata creation, and quality checks.' This shows you understand that good data management requires effort but frames it as integral to research, not an add-on.",
               "For projects with actual costs, detail them: Repository fees (though MOST Wiedzy is free for Gdańsk Tech researchers), extra cloud storage beyond institutional allocation (rare but possible for very large datasets), specialized software licenses for data management (usually covered by university licenses), training time for team members unfamiliar with data management practices. Most projects at Gdańsk Tech don't need extra budget, but acknowledging time as a resource is important.",
               "Leverage institutional resources to minimize costs: 'Gdańsk Tech provides: MOST Wiedzy repository (no cost), Office 365 storage (1TB included), library support for data management planning, IT support for technical issues, and training workshops on FAIR principles.' Showing awareness of these resources demonstrates good planning. The message is: data management is achievable within normal research operations when you use available support effectively."
          ]
     }
}