# DMP ART: Three Categories of Review Comments
# Based on analysis of DMP reviews at Gdańsk University of Technology

# CATEGORY 1: NEWCOMER GUIDANCE (Educational, descriptive comments)
NEWCOMER_GUIDANCE = {
    "1.1": [
        "In this first question, you should think about every stage of your research project. Take the research activities plan from your proposal and consider what kind of data you will collect at each stage. For example: Will you conduct experiments that generate measurement data? Will you run simulations that produce output files? Will you use existing datasets from other researchers? Be specific about each data source and collection method, as this helps plan for proper data management throughout the project.",
        
        "When describing data collection, distinguish between different types of data generation. Experimental data comes from physical experiments or observations (like measurements from sensors, microscopy images, or survey responses). Simulation data is generated by computational models (like CFD results, molecular dynamics trajectories, or economic models). Each type requires different management approaches, so please specify which types you'll work with.",
        
        "For projects involving external partners or subcontractors (common in social sciences or market research), explain how data will be collected by third parties. Include: Who will collect the data? What are their qualifications? How will they transfer data to you securely? What quality checks will you perform on received data? This is especially important for survey research or clinical trials."
    ],
    
    "1.2": [
        "This section requires you to be specific about three aspects of your data: types, formats, and volumes. For types, think about whether you'll have numerical data (measurements, calculations), textual data (interviews, documents), images (microscopy, photographs), or other media. For formats, specify the exact file types you'll use - for example, CSV files for tabular data, TIFF for microscopy images, MP4 for video recordings. For volumes, estimate the total storage space needed - even rough estimates like '50-100 GB' are better than vague terms like 'large amounts of data.'",
        
        "Data volume estimation might seem difficult at the project planning stage, but it's crucial for budgeting storage and backup resources. Consider: How many experiments/simulations will you run? What's the size of a typical output file? Will you keep all raw data or only processed results? A simple calculation example: 100 experiments × 500 MB per experiment = 50 GB total. Remember to include temporary files and intermediate processing results in your estimates.",
        
        "When listing data formats, think about long-term accessibility. Proprietary formats (like .xlsx or .mat files) may not be readable in 10 years. Whenever possible, plan to also save data in open formats: use CSV instead of Excel for tables, use TIFF or PNG instead of proprietary image formats, use plain text or PDF/A for documents. This ensures your data remains accessible long after your project ends."
    ],
    
    "2.1": [
        "Metadata is essentially 'data about data' - information that helps others (and future you!) understand and use your datasets. Think of metadata in layers: File-level metadata (creation date, creator, file format), Dataset-level metadata (what the data represents, how it was collected, units of measurement), and Project-level metadata (research context, related publications, funding information). Good metadata makes your data FAIR - Findable, Accessible, Interoperable, and Reusable.",
        
        "Documentation goes hand-in-hand with metadata but serves a different purpose. While metadata is often structured and machine-readable, documentation is typically human-readable text that explains your data in detail. This includes: README files explaining file structures and naming conventions, codebooks defining variables and their values, methodology documents describing experimental procedures, and data processing workflows showing how raw data was transformed. Think of documentation as the instruction manual for your data.",
        
        "For metadata standards, different research fields have different conventions. In general research, DataCite is widely accepted. For social sciences, consider DDI (Data Documentation Initiative). For geospatial data, use ISO 19115. For biodiversity data, Darwin Core is standard. If you're unsure which standard to use, DataCite is a safe choice that covers basic metadata needs. The key is to be consistent and comprehensive in your metadata creation."
    ],
    
    "2.2": [
        "Data quality control is about ensuring your data is accurate, complete, and reliable. This isn't the same as data security (which protects against unauthorized access) - quality control protects against errors and inconsistencies. Start by defining what 'good quality' means for your data: acceptable ranges for measurements, required fields that must be filled, consistency rules between related data points. Then describe how you'll check these criteria.",
        
        "Quality control procedures should be built into your research workflow, not added as an afterthought. For experimental data: calibrate instruments before use, record calibration results, use control samples, document any anomalies. For survey data: validate responses against logical rules, check for missing data patterns, identify potential outliers. For computational data: verify input parameters, validate outputs against known solutions, document software versions and settings.",
        
        "Document your quality control process as thoroughly as your actual data. When errors or issues are found, record: what the problem was, when it was discovered, how it was resolved, whether any data needed to be excluded. This creates an audit trail that enhances the credibility of your research. Consider using version control for datasets that change over time, just as you would for software code."
    ],
    
    "3.1": [
        "Storage and backup are different concepts that work together to protect your data. Storage is where you actively work with your data (like your office computer or a shared network drive). Backup is a separate copy kept for recovery purposes. At Gdańsk Tech, you have access to Office 365 A1 for faculty, which includes OneDrive cloud storage - this can serve as both working storage and off-site backup. The key is to have multiple copies in different locations.",
        
        "A good backup strategy follows the 3-2-1 rule: Keep 3 copies of important data (1 primary and 2 backups), Store copies on 2 different storage media (e.g., local drive and cloud storage), Keep 1 copy off-site (OneDrive fulfills this). Also important is backup frequency - active research data should be backed up daily, while completed datasets might only need weekly backups. Automate this process where possible to ensure consistency.",
        
        "Don't forget about metadata and documentation in your backup strategy! These files are just as important as your actual data. Store them together with your data so that the complete package is always backed up together. Also consider access permissions - who else needs to access the data during the project? Set up shared folders appropriately, but maintain control over who can modify vs. just read the data."
    ],
    
    "3.2": [
        "Data security protects your data from unauthorized access, corruption, or loss due to cyber threats. The level of security needed depends on data sensitivity. For most research data, basic security includes: password-protected computers, encrypted storage devices, secure file transfer methods (not email attachments!), and access controls on shared folders. For sensitive data (personal information, confidential business data), you'll need additional measures.",
        
        "Think about data security throughout the data lifecycle. During collection: use encrypted connections for online surveys, password-protect mobile devices used in field work. During storage: enable encryption on hard drives, use strong passwords, keep software updated. During sharing: use secure file transfer protocols (SFTP) or encrypted cloud services, never send sensitive data via unencrypted email. During disposal: securely wipe drives that contained sensitive data.",
        
        "Access control is a critical security component often overlooked. Define clearly: Who needs access to which data? What level of access do they need (read-only, edit, delete)? How long do they need access? Document these decisions and review them regularly. For collaborative projects, consider using institutional solutions that allow centralized access management rather than sharing passwords."
    ],
    
    "4.1": [
        "GDPR (General Data Protection Regulation) applies whenever you process personal data of EU residents. Personal data is any information that can identify a person - this includes obvious things like names and addresses, but also email addresses, IP addresses, voice recordings, photos, and even pseudonymized data if individuals can be re-identified. If your research involves humans in any way, carefully consider whether you're collecting personal data.",
        
        "If you're not processing personal data, state this clearly: 'This project will not process any personal data. All data collected will be from instruments/simulations/non-human sources.' But be certain - even metadata can contain personal information (like researcher names). If you ARE processing personal data, you need: a legal basis for processing (usually consent or legitimate interest), clear consent procedures, data minimization practices, and defined retention periods.",
        
        "For research involving personal data, consult with your institution's Data Protection Officer early in the planning process. They can help you: determine if your research needs a Data Protection Impact Assessment, design appropriate consent forms, plan for data subject rights (access, correction, deletion), and ensure your data flows comply with GDPR. Remember, GDPR compliance isn't just bureaucracy - it protects research participants and enhances research integrity."
    ],
    
    "4.2": [
        "Intellectual property (IP) and data ownership can be complex in research settings. By default, data created by employees during their work belongs to the employer (Gdańsk Tech), but there are exceptions and nuances. For single-institution projects, it's usually straightforward. For collaborative projects, especially with industry partners, data ownership must be clearly defined in consortium agreements before the project starts.",
        
        "Data licensing determines how others can use your data after publication. The choice of license is crucial for enabling reuse while protecting your interests. CC BY 4.0 (Attribution) is recommended for maximum openness - others can use your data for any purpose if they cite you. CC BY-SA (Share-Alike) requires derivative works to use the same license. CC BY-NC excludes commercial use. For data, avoid licenses with 'No Derivatives' clauses as they prevent most reuse.",
        
        "Consider all data sources when addressing IP rights. If you're using existing datasets, check their licenses - you may be restricted in how you can share derivative data. If your research might lead to patents, you may need to embargo data release until patent applications are filed. If you're using software to generate data, the software license might affect data rights. Document all these considerations clearly in your DMP."
    ],
    
    "5.1": [
        "Data sharing timeline is crucial for planning and compliance with funder requirements. 'As soon as possible' is too vague - specify concrete triggers or dates. Common approaches: 'Upon publication of associated article' (ensures you get credit first), 'At project end (March 2026)' (specific date), 'After 12-month embargo for patent application' (justified delay). Consider breaking large datasets into portions that can be released at different times.",
        
        "How you share data is as important as when. Repository upload is the gold standard - data gets persistent identifiers, standardized metadata, and long-term preservation. 'Available on request' is discouraged as it's not truly open and depends on your continued availability. If some data can't be openly shared (privacy, commercial sensitivity), explain what can be shared and how others can request access to restricted portions.",
        
        "Don't forget to repeat the license information from section 4.2 here! This is one of the most common oversights in DMPs. Also consider: Will you share raw data, processed data, or both? Will you include analysis scripts? How will you handle different versions if data is updated? What support can you provide to data users? Clear communication about what's shared and how to use it enhances the impact of your research."
    ],
    
    "5.2": [
        "Not all data needs eternal preservation - selection is key. Priority for preservation typically goes to: data underlying published results (for reproducibility), unique or hard-to-reproduce data (observations of rare events), data with long-term value beyond the original project, and clean, well-documented datasets. Raw data might be too large to preserve entirely - consider preserving processed data plus detailed processing methods instead.",
        
        "Repository choice affects data discoverability and longevity. MOST Wiedzy (Bridge of Data) is Gdańsk Tech's institutional repository and should be your first choice - it's CoreTrustSeal certified, ensures long-term preservation, and enhances institutional visibility. For discipline-specific needs, consider: Zenodo for general scientific data, GenBank for genetic sequences, PDB for protein structures. Domain repositories often provide specialized metadata and tools.",
        
        "Long-term preservation isn't just about storage - it's about ensuring data remains understandable and usable. This requires: comprehensive documentation preserved with the data, use of open file formats that will remain readable, sufficient metadata for discovery and understanding, and persistent identifiers for reliable citation. Most repositories guarantee preservation for at least 10 years, but plan for your data to outlive you."
    ],
    
    "5.3": [
        "Software requirements are often underestimated but are crucial for data reuse. Don't just list software names - include version numbers, as software behavior can change between versions. For example: 'Python 3.8+ with numpy 1.21.0, pandas 1.3.0' is much better than just 'Python'. For proprietary software like MATLAB or SPSS, mention the toolboxes or modules required. Consider whether someone in 10 years will have access to the same software.",
        
        "Think beyond the primary software to the complete environment needed to use your data. This includes: operating system requirements (some software is platform-specific), hardware requirements (especially for large datasets or intensive processing), additional libraries or dependencies, and configuration files or settings. If you've written custom analysis scripts, these are part of your data product and should be shared too.",
        
        "Documentation for software tools should include not just what's needed, but HOW to use them with your data. Provide: installation instructions (especially for complex environments), example commands or workflows for common analyses, explanation of any custom scripts or modifications, and contact information for questions (though don't rely on personal availability long-term). Consider creating a 'Getting Started' guide that walks through a simple analysis example."
    ],
    
    "5.4": [
        "Persistent identifiers like DOIs (Digital Object Identifiers) are the cornerstone of FAIR data. They provide a permanent link to your data that won't break when websites reorganize or institutions change systems. At Gdańsk Tech, DOIs are automatically assigned when you deposit data in MOST Wiedzy repository. The timing is important: DOIs are assigned upon deposit, so factor this into your data sharing timeline.",
        
        "Beyond repository-assigned DOIs, consider identifiers throughout your research workflow. Use consistent naming schemes for samples, experiments, or study participants. If your field has standard identifier systems (like CAS numbers for chemicals or taxonomic IDs for species), use them. Document your identification scheme so others can understand the relationship between identifiers and real-world entities. This internal consistency enhances data usability.",
        
        "Plan how you'll use DOIs in practice. Include them in: data availability statements in publications, citations in your reference list (data should be cited like publications), metadata for related outputs, and project documentation. If you have a complex dataset with multiple components, decide whether each component needs its own DOI or if one DOI for the collection suffices. Remember: DOIs are for finished, published data - use version control for work in progress."
    ],
    
    "6.1": [
        "Data management responsibilities must be clearly assigned to specific people, not just roles. While the Principal Investigator has overall responsibility, day-to-day tasks should be distributed. Consider: Who creates metadata during data collection? Who performs quality checks? Who handles backup procedures? Who prepares data for publication? Clear assignment prevents tasks from falling through cracks and ensures accountability.",
        
        "FAIR data principles require active management throughout the project. The PI typically ensures: data is Findable through proper metadata and identifiers, Accessible through repository deposit, Interoperable through standard formats and vocabularies, and Reusable through clear licensing and documentation. But these high-level responsibilities translate to specific tasks that can be delegated to team members with appropriate skills.",
        
        "Succession planning is often overlooked but crucial for long-term data availability. What happens if the responsible person leaves the project? Document: where all data is stored, passwords and access procedures (in secure institutional systems, not personal records), key decisions made about data management, and contact information for data questions. Consider appointing a deputy who understands the data management procedures."
    ],
    
    "6.2": [
        "Data management requires resources, even if it doesn't always require additional budget. Time is the most important resource - estimate what percentage of research time will go to data management tasks. Typically, this is 5-10% of total research effort, more for data-intensive projects. Be realistic: good data management takes time but saves more time later by preventing data loss and easing publication preparation.",
        
        "Financial resources may be needed for: repository fees (though many are free for basic use), cloud storage beyond institutional allocations, specialized software licenses, training for team members on data management, or time for dedicated data management staff on large projects. If your funder allows, include these in your budget. If not, explain how you'll cover these needs with existing resources.",
        
        "Consider non-monetary resources that support FAIR data: institutional infrastructure (like MOST Wiedzy repository and Office 365 storage), library support services for data management, IT support for technical issues, and training opportunities for team members. Acknowledging these resources shows you've thought comprehensively about implementation. Also consider the cost of NOT managing data properly - lost data can mean repeated experiments and delayed publications."
    ]
}

# CATEGORY 2: WRONG/MISSING INFORMATION (Corrections and discipline-specific issues)
WRONG_MISSING_INFO = {
    "general": [
        # Storage and backup issues
        "Dropbox is not recommended for research data storage during the project due to GDPR concerns and lack of institutional control. Instead, use the institutional Office 365 A1 for faculty license which provides OneDrive storage with proper data governance and EU data residency compliance.",
        
        "Google Drive free edition lacks proper version control and audit trails required for research data. Use institutional solutions like Office 365 OneDrive or dedicated research data storage that provides proper access controls and activity logging.",
        
        "Personal external drives as the sole backup solution is insufficient. They can fail, be lost, or stolen. Always maintain at least one off-site backup using institutional cloud storage (OneDrive) or approved research data services.",
        
        # Repository issues
        "Generic cloud storage (Dropbox, Google Drive, personal websites) are not appropriate for long-term data preservation. Use proper research data repositories like MOST Wiedzy (Bridge of Data), Zenodo, or discipline-specific repositories that provide persistent identifiers and commit to long-term preservation.",
        
        "ResearchGate and Academia.edu are academic social networks, not data repositories. They don't provide DOIs, long-term preservation guarantees, or proper metadata standards. Use them for visibility but deposit data in proper repositories.",
        
        # License issues
        "'Available upon request' is not truly open access and depends on your continued availability. Deposit data in a repository where it can be accessed directly. If restrictions are necessary, use repository access controls rather than manual gatekeeping.",
        
        "CC BY-NC (Non-Commercial) licenses can severely limit data reuse, as 'commercial' is poorly defined and may exclude legitimate research uses. CC BY 4.0 is recommended unless there are specific reasons to restrict use.",
        
        # GDPR issues
        "Stating 'GDPR not applicable' without justification is insufficient. You must explicitly confirm that no personal data will be processed, including in metadata, collaboration records, or incidental collection.",
        
        "Anonymization is not the same as pseudonymization. Anonymized data cannot be re-identified by anyone, while pseudonymized data can be re-identified with additional information. GDPR applies to pseudonymized data but not truly anonymized data.",
    ],
    
    # Discipline-specific issues
    "biomedical_engineering": [
        "For research involving human subjects, there's no mention of bioethical committee approval. Include the committee name, approval number, and confirm that data management procedures were part of the ethical review.",
        
        "Medical imaging data requires special consideration for anonymization. DICOM files contain metadata that can include patient information - describe your anonymization workflow, such as using tools like DicomCleaner or DICOM Anonymizer.",
        
        "For wearable sensor data from human subjects, consider that movement patterns and physiological signals can be identifying. Describe how you'll protect participant privacy while maintaining data utility for research.",
    ],
    
    "chemical_sciences": [
        "Chemical structure data should be shared in standard formats like SDF or CIF, not just as images in publications. This enables computational reuse and structure searching.",
        
        "For spectroscopy data (NMR, MS, IR), raw data files should be preserved alongside processed spectra. Include instrument parameters and processing details to enable reanalysis.",
        
        "Safety data sheets (SDS) and risk assessments are part of research documentation. While not usually shared publicly, they should be preserved with internal project records.",
    ],
    
    "information_technology": [
        "Source code is research output too. Plan to share code in GitHub/GitLab with proper licensing (consider MIT or Apache 2.0 for software, not Creative Commons which is designed for content).",
        
        "For machine learning projects, sharing only the trained model is insufficient. Include training data (or references to it), hyperparameters, training scripts, and evaluation metrics for reproducibility.",
        
        "Database schemas and API documentation are essential metadata for software projects. Don't just dump SQL files - provide entity-relationship diagrams and data dictionaries.",
    ],
    
    "civil_engineering": [
        "Geospatial data requires coordinate system information. Always specify the CRS (Coordinate Reference System) used, such as EPSG:2180 for Polish National Grid.",
        
        "For structural monitoring data, time synchronization between sensors is critical. Describe your time reference (GPS time, NTP server) and synchronization accuracy.",
        
        "Building Information Modeling (BIM) files in proprietary formats (.rvt, .pln) should be accompanied by open IFC (Industry Foundation Classes) exports for long-term accessibility.",
    ],
    
    "environmental_engineering": [
        "Environmental monitoring data must include quality assurance/quality control (QA/QC) information. Describe calibration procedures, blank samples, duplicates, and spike recoveries.",
        
        "For field sampling data, GPS coordinates alone are insufficient. Include site descriptions, photos, weather conditions, and any factors that might affect data interpretation.",
        
        "Time series environmental data should specify measurement intervals, any gaps in data, and how missing values are handled. Irregular sampling requires special documentation.",
    ],
    
    "materials_engineering": [
        "Microscopy images require scale bars embedded in images AND pixel size in metadata. Include imaging parameters: voltage, magnification, detector type, sample preparation method.",
        
        "For mechanical testing data, raw force-displacement curves should be preserved, not just calculated properties. Include test standards followed (e.g., ISO, ASTM) and any deviations.",
        
        "X-ray diffraction data should be shared in standard formats (CIF for structures, raw diffractograms in xy or XRDML format), not just as peak lists or images.",
    ],
    
    "economics_finance": [
        "Financial data often has redistribution restrictions. If using commercial databases (Bloomberg, Refinitiv), check license terms - you usually can't reshare raw data but can share derived datasets.",
        
        "For survey data in economics, sampling weights and survey design must be documented. Include questionnaires in original language and English translation if applicable.",
        
        "Time series economic data should specify: frequency, seasonal adjustment methods, data transformations applied, and original sources for transparency.",
    ],
    
    "architecture_urban_planning": [
        "Architectural drawings in proprietary CAD formats (.dwg) should be accompanied by open formats (DXF, PDF/A) for long-term accessibility.",
        
        "For urban planning data involving public participation, careful anonymization is needed. Geographic data can be identifying when combined with demographic information.",
        
        "3D city models should include Level of Detail (LoD) specification and coordinate systems. Consider CityGML for standardized 3D urban data sharing.",
    ],
    
    "mechanical_engineering": [
        "CAD models in native formats (.prt, .asm) are often version-specific. Include neutral formats (STEP, IGES) and PDF drawings for basic access.",
        
        "For CFD simulation data, mesh files can be enormous. Consider sharing mesh generation parameters and scripts rather than mesh files themselves.",
        
        "Experimental vibration data should include sensor calibration certificates, mounting methods, and measurement chain documentation for proper interpretation.",
    ],
    
    "automation_electronics": [
        "For control system data, include sampling rates and anti-aliasing filter specifications. Real-time data has different requirements than post-processed data.",
        
        "Circuit designs should include both schematic and PCB layout files. Consider open formats (KiCad) alongside proprietary ones (Altium, Eagle).",
        
        "FPGA/embedded system projects should specify tool versions precisely - bitstreams are often not portable between tool versions.",
    ],
    
    "mathematics": [
        "Mathematical proofs in paper form should be accompanied by any computational verification code. Consider proof assistants (Coq, Lean) for formal proofs.",
        
        "For numerical computations, floating-point precision matters. Specify whether calculations used single, double, or arbitrary precision arithmetic.",
        
        "Random number generation must be documented: algorithm used, seeds for reproducibility, and any statistical tests performed on generators.",
    ],
    
    "physical_sciences": [
        "Experimental physics data must include uncertainty estimates. Describe how uncertainties were calculated (statistical, systematic, combined).",
        
        "For astronomy/astrophysics data, include observation logs: timestamps (specify time standard), weather conditions, instrument settings, calibration frames.",
        
        "Simulation parameters in physics must be complete: initial conditions, boundary conditions, numerical methods, convergence criteria.",
    ]
}

# CATEGORY 3: READY-TO-USE SENTENCES (Copy-paste statements)
READY_TO_USE_SENTENCES = {
    "repository_statements": [
        "MOST Wiedzy Open Research Data Catalog is CoreTrustSeal certified, which confirms compliance with international standards for trustworthy data repositories, ensuring long-term preservation and accessibility of deposited datasets.",
        
        "Data will be deposited in MOST Wiedzy (Bridge of Data), Gdańsk University of Technology's institutional repository, which provides DOI assignment, standardized metadata, and commits to minimum 10-year preservation period.",
        
        "Zenodo, operated by CERN, provides free data archiving up to 50GB per dataset with automatic DOI assignment, versioning support, and integration with GitHub for software preservation.",
        
        "The repository ensures data preservation for a minimum of 10 years after deposit, with regular integrity checks, migration strategies for obsolete formats, and succession planning for long-term sustainability.",
    ],
    
    "license_statements": [
        "Data will be shared under Creative Commons Attribution 4.0 International (CC BY 4.0) license, allowing any use including commercial purposes, provided appropriate credit is given and changes are indicated.",
        
        "The CC BY 4.0 license aligns with Plan S requirements and Horizon Europe open science mandates, ensuring maximum reuse potential while protecting attribution rights.",
        
        "For mixed content (data, code, documentation), appropriate licenses will be applied: CC BY 4.0 for data and documentation, MIT license for software code, ensuring clarity of reuse terms.",
    ],
    
    "gdpr_statements": [
        "This project will not process any personal data as defined by GDPR. All data will be collected from instruments, simulations, or publicly available sources without any connection to identifiable individuals.",
        
        "Personal data processing will comply with GDPR Article 6(1)(a) - consent, and Article 9(2)(a) for special categories. Participants will provide informed consent through ethics-committee-approved forms.",
        
        "Data minimization principle will be followed: only data necessary for research objectives will be collected, personal identifiers will be replaced with study codes at earliest opportunity, and original consent forms stored separately from research data.",
        
        "Participants' rights under GDPR (access, rectification, erasure, portability) will be ensured through documented procedures, with contact information provided in consent forms and study information sheets.",
    ],
    
    "storage_backup_statements": [
        "Research data will be stored on Gdańsk University of Technology infrastructure using our institutional Office 365 A1 for faculty license, which provides OneDrive cloud storage with automatic versioning and EU data residency.",
        
        "Backup strategy follows the 3-2-1 rule: 3 copies of data (working + 2 backups), on 2 different storage media (local drive + OneDrive cloud), with 1 copy off-site (OneDrive serves this purpose).",
        
        "Active research data will be backed up daily using automated OneDrive sync, completed datasets backed up weekly, with backup integrity verified monthly through restoration tests.",
        
        "Access control will be managed through Office 365 groups: PI has full access, research team has read/write access to relevant folders, external collaborators receive read-only access to specific datasets as needed.",
    ],
    
    "fair_statements": [
        "Data will be made FAIR through: (F)indable via DOIs and rich metadata, (A)ccessible through standard protocols, (I)nteroperable using standard formats and vocabularies, (R)eusable with clear licenses and documentation.",
        
        "Metadata will follow DataCite schema 4.4, including: creators with ORCID iDs, descriptive title and abstract, subject keywords from controlled vocabularies, related publications and grants, technical details for reuse.",
        
        "To ensure interoperability, data will use standard formats: CSV with UTF-8 encoding for tabular data, NetCDF for multidimensional arrays, GeoJSON for spatial data, all with documented schemas.",
    ],
    
    "software_citation_statements": [
        "All software used for data processing will be cited including version numbers: Python 3.8.10 (Van Rossum & Drake, 2009), NumPy 1.21.0 (Harris et al., 2020), Pandas 1.3.0 (McKinney, 2010).",
        
        "Custom analysis scripts will be deposited alongside data with documentation, allowing full reproduction of data processing workflows from raw data to published results.",
        
        "Software environment will be documented using requirements.txt for Python, renv for R, or Docker containers for complex dependencies, ensuring computational reproducibility.",
    ],
    
    "quality_control_statements": [
        "Data quality control will include: range checks for physical plausibility, completeness checks for required fields, consistency checks between related variables, with all QC results documented in metadata.",
        
        "Instrument calibration will be performed before each measurement session, with calibration results recorded in experimental logbooks and included as metadata with datasets.",
        
        "Version control will track all changes to datasets: major versions for structural changes, minor versions for error corrections, with change logs documenting what changed and why.",
    ],
    
    "preservation_selection_statements": [
        "All data underlying published results will be preserved to ensure reproducibility. Raw data will be retained where feasible; otherwise, processed data with detailed processing workflows will be preserved.",
        
        "Selection for long-term preservation prioritizes: uniqueness (cannot be recreated), relevance (likely reuse value), quality (well-documented and validated), and size (within repository limits).",
        
        "Data will be packaged for preservation with all necessary components: primary data files, metadata in standard schemas, documentation including READMEs and codebooks, and any custom software needed for data use.",
    ],
    
    "timeline_statements": [
        "Data will be made publicly available upon publication of associated research articles, with DOIs reserved in advance to include in data availability statements.",
        
        "For patentable results, data release will be embargoed for maximum 12 months from project completion to allow patent filing, with embargo reason clearly stated in repository metadata.",
        
        "Progressive data release is planned: metadata immediately upon deposit, preliminary data after conference presentation, complete datasets upon journal publication, ensuring timely access while protecting publication priority.",
    ],
    
    "cost_resource_statements": [
        "Data management activities are estimated to require 5% of total researcher time throughout the project, primarily for metadata creation, quality control, and preparation for archiving.",
        
        "No additional costs are anticipated as data storage uses institutional infrastructure (Office 365), repository deposit is free (MOST Wiedzy), and all required software uses open-source solutions.",
        
        "The Principal Investigator will dedicate 2 hours monthly to oversee data management implementation, with team members responsible for day-to-day tasks as part of good research practice.",
    ]
}

# Implementation function to get relevant comments based on section and content
def get_relevant_comments(section_id, content, user_type="newcomer", discipline=None):
    """
    Returns relevant comments based on section, content analysis, and user type
    
    Args:
        section_id: DMP section (e.g., "1.1", "2.1")
        content: Current content in the section
        user_type: "newcomer", "wrong_missing", or "ready_to_use"
        discipline: Research discipline for specific feedback
    
    Returns:
        List of relevant comments
    """
    comments = []
    content_lower = content.lower()
    
    if user_type == "newcomer":
        # Return educational guidance for the section
        if section_id in NEWCOMER_GUIDANCE:
            comments.extend(NEWCOMER_GUIDANCE[section_id])
    
    elif user_type == "wrong_missing":
        # Check for common errors
        relevant_comments = []
        
        # Check general issues
        for comment in WRONG_MISSING_INFO["general"]:
            keywords = ["dropbox", "google drive", "researchgate", "upon request", "gdpr not applicable"]
            if any(keyword in content_lower for keyword in keywords):
                relevant_comments.append(comment)
        
        # Check discipline-specific issues
        if discipline and discipline in WRONG_MISSING_INFO:
            relevant_comments.extend(WRONG_MISSING_INFO[discipline])
        
        comments = relevant_comments
    
    elif user_type == "ready_to_use":
        # Return ready-to-use sentences based on section topic
        section_topics = {
            "1.1": ["collection_statements"],
            "1.2": ["format_statements"],
            "2.1": ["fair_statements", "metadata_statements"],
            "2.2": ["quality_control_statements"],
            "3.1": ["storage_backup_statements"],
            "3.2": ["security_statements"],
            "4.1": ["gdpr_statements"],
            "4.2": ["license_statements"],
            "5.1": ["timeline_statements", "license_statements"],
            "5.2": ["repository_statements", "preservation_selection_statements"],
            "5.3": ["software_citation_statements"],
            "5.4": ["identifier_statements"],
            "6.1": ["responsibility_statements"],
            "6.2": ["cost_resource_statements"]
        }
        
        # Get relevant statement categories for this section
        if section_id in section_topics:
            for topic in section_topics[section_id]:
                if topic in READY_TO_USE_SENTENCES:
                    comments.extend(READY_TO_USE_SENTENCES[topic])
    
    return comments

# Export for integration into app.py
if __name__ == "__main__":
    print("DMP Review Comments - Three Categories")
    print("=====================================")
    print(f"Newcomer Guidance: {sum(len(v) for v in NEWCOMER_GUIDANCE.values())} comments")
    print(f"Wrong/Missing Info: {sum(len(v) for v in WRONG_MISSING_INFO.values())} comments")
    print(f"Ready-to-use Sentences: {sum(len(v) for v in READY_TO_USE_SENTENCES.values())} sentences")
